#!/usr/bin/env python3
"""
Script completo para compara√ß√£o de defesas em Federated Learning.
Refatorado para separar Prepara√ß√£o, Execu√ß√£o e An√°lise.

- Modo Prepara√ß√£o (--run, --resume_dir): Gera/valida configs YAML e cria 
  scripts de shell (run_job_N.sh) para os executores.
- Modo An√°lise (--analyze_dir): Carrega resultados, processa e gera CSVs.

A execu√ß√£o em si √© delegada aos scripts de job, que chamam simulate_fl.py.
"""

import sys
import subprocess
import yaml
import json
import time
import pandas as pd
from pathlib import Path
from datetime import datetime
import numpy as np
import argparse
import re
import ast
import os  # Adicionado para chmod

# Adicionar diret√≥rio raiz ao path, se necess√°rio
# (Assumindo que fl_simulator est√° no mesmo diret√≥rio ou no PYTHONPATH)
# sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# A verifica√ß√£o de import do simulador foi movida para dentro do main (modo prepara√ß√£o)


def slugify(text):
    """Converte um texto para um formato 'slug' seguro para nomes de arquivos."""
    text = text.lower()
    text = re.sub(r'\s+', '_', text)  # Substitui espa√ßos por underscores
    text = re.sub(r'[^\w\-]+', '', text) # Remove caracteres n√£o alfanum√©ricos
    return text


def ensure_list(x):
    """Garantir que o valor seja uma lista (para varrer varia√ß√µes)."""
    if x is None:
        return [None]
    return x if isinstance(x, list) else [x]


class ResultAnalyzer:
    """
    Classe para carregar e analisar resultados de experimentos.
    """
    def __init__(self, simulation_dir):
        """
        Inicializa o analisador com o diret√≥rio base da simula√ß√£o.

        Args:
            simulation_dir (str or Path): O diret√≥rio da simula√ß√£o contendo a pasta 'results'.
        """
        self.base_dir = Path(simulation_dir)
        self.results_dir = self.base_dir / 'results'

        if not self.results_dir.exists():
            raise FileNotFoundError(f"Diret√≥rio de resultados n√£o encontrado: {self.results_dir}")

        self.all_results = {}
        self.metadata = {}
        print(f"üìä Analisador iniciado para o diret√≥rio: {self.base_dir}")

    def load_results(self):
        """
        Carrega os resultados a partir de um arquivo de resumo JSON ou varrendo os diret√≥rios.
        """
        summary_file = self.results_dir / 'complete_results.json'
        if summary_file.exists():
            print(f"üìÑ Carregando resultados do arquivo de resumo: {summary_file}")
            with open(summary_file, 'r') as f:
                summary_data = json.load(f)
            self.all_results = summary_data.get('results', {})
            self.metadata = summary_data.get('experiment_info', {})
            print(f"‚úÖ {len(self.all_results)} resultados carregados.")
        else:
            print("‚ö†Ô∏è Arquivo de resumo n√£o encontrado. Varrendo diret√≥rios de experimentos...")
            for exp_dir in self.results_dir.iterdir():
                if exp_dir.is_dir():
                    metrics_file = exp_dir / 'metrics.json'
                    config_file = exp_dir / 'config.yaml'

                    if metrics_file.exists() and config_file.exists():
                        try:
                            with open(metrics_file, 'r') as f:
                                metrics = json.load(f)
                            with open(config_file, 'r') as f:
                                config = yaml.safe_load(f)
                            
                            malicious_clients = []
                            malicious_file = exp_dir / 'malicious_clients.json'
                            if malicious_file.exists():
                                with open(malicious_file, 'r') as f:
                                    malicious_data = json.load(f)
                                    malicious_clients = malicious_data.get('malicious_clients', [])

                            # Extrai metadados do config.yaml
                            exp_name = config.get('experiment', {}).get('name', exp_dir.name)
                            # Usa o nome da defesa do bloco experiment; fallback seguro
                            defense_name = config.get('experiment', {}).get('defense_name', 'Sem Defesa')

                            attack_rate = config.get('clients', {}).get('malicious_percentage', 0.0)
                            sel_frac = config.get('server', {}).get('selection_fraction', 0.0)
                            
                            # Listas de m√©tricas podem estar vazias se a simula√ß√£o falhou cedo
                            acc_list = metrics.get('accuracy') or []
                            loss_list = metrics.get('loss') or []
                            prec_list = metrics.get('precision') or []
                            rec_list = metrics.get('recall') or []
                            f1_list = metrics.get('f1') or []

                            # Determina se o experimento foi conclu√≠do com sucesso
                            total_rounds_config = config.get('experiment', {}).get('rounds')
                            rounds_executados = len(metrics.get('rounds', [])) if isinstance(metrics.get('rounds', []), list) else len(acc_list)
                            try:
                                total_rounds_config_int = int(total_rounds_config) if total_rounds_config is not None else None
                            except (TypeError, ValueError):
                                total_rounds_config_int = None
                            is_success = (total_rounds_config_int is not None) and (rounds_executados >= total_rounds_config_int)

                            self.all_results[exp_name] = {
                                'config_path': str(config_file),
                                'defense': defense_name,
                                'attack_rate': attack_rate,
                                'selection_fraction': sel_frac,
                                'selection_strategy': config.get('server', {}).get('selection_strategy'),
                                'dataset_name': config.get('dataset', {}).get('name'),
                                'non_iid': config.get('dataset', {}).get('non_iid'),
                                'dirichlet_alpha': config.get('dataset', {}).get('dirichlet_alpha'),
                                'model_type': config.get('model', {}).get('type'),
                                'local_epochs': config.get('model', {}).get('local_epochs'),
                                'batch_size': config.get('model', {}).get('batch_size'),
                                'learning_rate': config.get('model', {}).get('learning_rate'),
                                'rounds_total': config.get('experiment', {}).get('rounds'),
                                'num_clients': config.get('clients', {}).get('num_clients'),
                                'seed': config.get('experiment', {}).get('seed'),
                                'reuse_client_model': config.get('experiment', {}).get('reuse_client_model', False),
                                'keras_verbose': config.get('experiment', {}).get('keras_verbose', 0),
                                'malicious_clients': malicious_clients,
                                'metrics': metrics,
                                'final_accuracy': acc_list[-1] if acc_list else 0.0,
                                'final_loss': loss_list[-1] if loss_list else float('inf'),
                                'final_precision': prec_list[-1] if prec_list else 0.0,
                                'final_recall': rec_list[-1] if rec_list else 0.0,
                                'final_f1': f1_list[-1] if f1_list else 0.0,
                                'success': is_success,
                            }
                        except Exception as e:
                            print(f"‚ùå Erro ao processar o diret√≥rio {exp_dir.name}: {e}")
            print(f"‚úÖ Varredura conclu√≠da. Encontrados {len(self.all_results)} resultados.")
        
        if not self.all_results:
             print("‚ùå Nenhum resultado para analisar.")
             return False
        return True
    
    def run_full_analysis(self):
        """Executa o pipeline completo de an√°lise: carregar dados e processar."""
        # Evita revarrer se j√° est√° carregado
        if not self.all_results:
            if not self.load_results():
                print("An√°lise interrompida.")
                return

        if self.all_results:
            # Salva o resumo JSON consolidado
            self.save_results_summary(preloaded_results=self.all_results)
            
            # Instancia o processador para gerar DataFrame e gr√°ficos
            processor = ResultProcessor(self.all_results, self.results_dir)
            processor.generate_and_save_dataframe()
            processor.generate_final_metrics_summary()
        else:
            print("An√°lise interrompida.")

    def _derive_experiment_info(self, results: dict) -> dict:
        """Deriva metadados a partir dos resultados reais, evitando inconsist√™ncias."""
        defenses = sorted({v.get('defense') for v in results.values() if isinstance(v, dict)})
        attack_rates = sorted({float(v.get('attack_rate', 0.0)) for v in results.values() if isinstance(v, dict)})
        selection_fractions = sorted({float(v.get('selection_fraction', 0.0)) for v in results.values() if isinstance(v, dict)})
        rounds_candidates = [int(v.get('rounds_total')) for v in results.values() if isinstance(v, dict) and v.get('rounds_total')]
        rounds_val = max(rounds_candidates) if rounds_candidates else None
        return {
            'total_experiments': len(results),
            'defenses': defenses,
            'attack_rates': attack_rates,
            'selection_fractions': selection_fractions,
            'rounds': rounds_val,
            'timestamp': datetime.now().isoformat()
        }

    def save_results_summary(self, preloaded_results: dict | None = None):
        """Cria ou atualiza o arquivo de resumo JSON com todos os resultados.
        Se preloaded_results for fornecido, evita uma nova varredura do disco.
        """
        print("\nüíæ Salvando resumo final dos resultados...")

        if preloaded_results is not None:
            results = preloaded_results
        else:
            # Se n√£o pr√©-carregado, carrega agora
            if not self.all_results:
                self.load_results()
            results = self.all_results

        if not results:
            print("‚ö†Ô∏è Nenhum resultado encontrado para criar o resumo.")
            return

        experiment_info = self._derive_experiment_info(results)
        summary_data = {
            'experiment_info': experiment_info,
            'results': results
        }

        summary_path = self.results_dir / 'complete_results.json'
        with open(summary_path, 'w') as f:
            json.dump(summary_data, f, indent=2, default=str)
        print(f"üìÑ Resumo atualizado salvo em: {summary_path}")


class ResultProcessor:
    def __init__(self, all_results, results_dir):
        self.all_results = all_results
        self.results_dir = results_dir

    def generate_and_save_dataframe(self):
        """
        Converte os resultados carregados em um DataFrame Pandas e o salva em um arquivo CSV.
        """
        if not self.all_results:
            print("‚ö†Ô∏è Sem dados para criar o DataFrame.")
            return

        all_experiments_data = []
        for exp_id, exp_details in self.all_results.items():
            if not exp_details.get('success', False):
                continue

            metrics = exp_details.get("metrics", {})
            num_rounds = len(metrics.get("rounds", []))

            if num_rounds == 0:
                continue

            # Converte a lista de clientes maliciosos para uma string para f√°cil armazenamento no CSV
            malicious_clients_str = str(exp_details.get("malicious_clients", []))

            for i in range(num_rounds):
                # Usar .get() com um valor padr√£o seguro para evitar erros
                default_list = [None] * num_rounds
                default_bool_list = [False] * num_rounds

                try:
                    round_data = {
                        "experiment_id": exp_id,
                        "defense": exp_details.get("defense"),
                        "attack_rate": exp_details.get("attack_rate"),
                        "selection_fraction": exp_details.get("selection_fraction"),
                        "selection_strategy": exp_details.get("selection_strategy"),
                        "dataset_name": exp_details.get("dataset_name"),
                        "non_iid": exp_details.get("non_iid"),
                        "dirichlet_alpha": exp_details.get("dirichlet_alpha"),
                        "model_type": exp_details.get("model_type"),
                        "local_epochs": exp_details.get("local_epochs"),
                        "batch_size": exp_details.get("batch_size"),
                        "learning_rate": exp_details.get("learning_rate"),
                        "rounds_total": exp_details.get("rounds_total"),
                        "num_clients": exp_details.get("num_clients"),
                        "seed": exp_details.get("seed"),
                        "reuse_client_model": exp_details.get("reuse_client_model"),
                        "keras_verbose": exp_details.get("keras_verbose"),
                        "malicious_clients": malicious_clients_str,
                        "round": metrics.get("rounds")[i],
                        "accuracy": metrics.get("accuracy")[i],
                        "loss": metrics.get("loss")[i],
                        "precision": metrics.get("precision", default_list)[i],
                        "recall": metrics.get("recall", default_list)[i],
                        "f1_score": metrics.get("f1", default_list)[i],
                        "aggregation_time": metrics.get("aggregation_time", default_list)[i],
                        "round_time": metrics.get("round_time", default_list)[i],
                        "communication_bytes": metrics.get("communication_bytes", default_list)[i],
                        "selected_clients": str(metrics.get("selected_clients", default_list)[i]),
                        "aggregated_clients": str(metrics.get("aggregated_clients", default_list)[i]),
                        "model_updated": metrics.get("model_updated", default_bool_list)[i],
                        "final_accuracy": exp_details.get("final_accuracy"),
                        "final_loss": exp_details.get("final_loss"),
                        "final_precision": exp_details.get("final_precision"),
                        "final_recall": exp_details.get("final_recall"),
                        "final_f1_score": exp_details.get("final_f1"),
                        "success": exp_details.get("success"),
                    }
                    all_experiments_data.append(round_data)
                except (IndexError, TypeError, KeyError):
                    print(f"‚ö†Ô∏è  Aviso: Pulando round {i} do exp {exp_id} devido a dados ausentes/corrompidos.")
                    continue

        df = pd.DataFrame(all_experiments_data)
        df_path = self.results_dir / 'results_summary.csv'
        df.to_csv(df_path, index=False)
        print(f"\n‚úÖ DataFrame com {len(df)} linhas salvo em: {df_path}")

    def _calculate_filter_metrics(self, df_experiment):
        """
        Calcula as m√©tricas de classifica√ß√£o para a tarefa de filtragem de clientes.
        - Verdadeiro Positivo (TP): Cliente malicioso foi REJEITADO.
        - Verdadeiro Negativo (TN): Cliente honesto foi ACEITO.
        - Falso Positivo (FP): Cliente honesto foi REJEITADO.
        - Falso Negativo (FN): Cliente malicioso foi ACEITO.
        """
        # Converte a string de clientes maliciosos (que √© a mesma para todo o experimento) em um conjunto para busca r√°pida.
        try:
            malicious_clients_set = set(ast.literal_eval(df_experiment['malicious_clients'].iloc[0]))
        except (ValueError, SyntaxError):
            malicious_clients_set = set() # Lida com casos onde n√£o h√° clientes maliciosos

        tp, tn, fp, fn = 0, 0, 0, 0

        for _, row in df_experiment.iterrows():
            selected_clients = set(ast.literal_eval(row['selected_clients']))
            aggregated_clients = set(ast.literal_eval(row['aggregated_clients']))

            # Itera sobre todos os clientes que participaram da sele√ß√£o no round
            for client_id in selected_clients:
                is_malicious = client_id in malicious_clients_set
                was_aggregated = client_id in aggregated_clients

                if is_malicious and not was_aggregated:
                    tp += 1
                elif not is_malicious and was_aggregated:
                    tn += 1
                elif not is_malicious and not was_aggregated:
                    fp += 1
                elif is_malicious and was_aggregated:
                    fn += 1
        
        # C√°lculo das m√©tricas de classifica√ß√£o
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        return {
            'filter_accuracy': accuracy,
            'filter_precision': precision,
            'filter_recall': recall,
            'filter_f1_score': f1_score,
        }

    def _calculate_update_decision_metrics(self, df_experiment):
        """
        Avalia a decis√£o do servidor de atualizar ou n√£o o modelo.
        - Cen√°rio Ideal: Atualizar (True) se NENHUM malicioso foi agregado; N√£o atualizar (False) se ALGUM malicioso foi agregado.
        - TP: Decis√£o correta de N√ÉO atualizar (havia maliciosos agregados).
        - TN: Decis√£o correta de ATUALIZAR (n√£o havia maliciosos agregados).
        - FP: Decis√£o errada de N√ÉO atualizar (n√£o havia maliciosos agregados).
        - FN: Decis√£o errada de ATUALIZAR (havia maliciosos agregados).
        """
        try:
            malicious_clients_set = set(ast.literal_eval(df_experiment['malicious_clients'].iloc[0]))
        except (ValueError, SyntaxError):
            malicious_clients_set = set()

        tp, tn, fp, fn = 0, 0, 0, 0
        
        for _, row in df_experiment.iterrows():
            try:
                aggregated_clients = set(ast.literal_eval(row['aggregated_clients']))
            except (ValueError, SyntaxError):
                continue

            malicious_in_aggregation = not malicious_clients_set.isdisjoint(aggregated_clients)
            model_was_updated = row['model_updated']

            if malicious_in_aggregation and not model_was_updated:
                tp += 1
            elif not malicious_in_aggregation and model_was_updated:
                tn += 1
            elif not malicious_in_aggregation and not model_was_updated:
                fp += 1
            elif malicious_in_aggregation and model_was_updated:
                fn += 1

        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        
        return {
            'update_decision_accuracy': accuracy,
            'update_decision_true_positives': tp,
            'update_decision_true_negatives': tn,
            'update_decision_false_positives': fp,
            'update_decision_false_negatives': fn,
        }

    def generate_final_metrics_summary(self):
        """
        L√™ o CSV de resumo detalhado, agrega os dados por experimento e salva
        um novo CSV com as m√©tricas finais, ideal para tabelas de disserta√ß√£o.
        """
        detailed_csv_path = self.results_dir / 'results_summary.csv'
        if not detailed_csv_path.exists():
            print(f"‚ö†Ô∏è  Arquivo {detailed_csv_path} n√£o encontrado. Pulando a gera√ß√£o do sum√°rio final.")
            return

        print("\nüìÑ Gerando CSV com m√©tricas finais agregadas...")
        df = pd.read_csv(detailed_csv_path)
        
        # Agrupa o dataframe por cada experimento √∫nico
        grouped_experiments = df.groupby('experiment_id')
        
        final_results = []

        for exp_id, df_exp in grouped_experiments:
            final_row = df_exp.iloc[-1] # Pega a √∫ltima linha para m√©tricas finais do modelo

            # 1. M√©tricas do Filtro de Clientes
            filter_metrics = self._calculate_filter_metrics(df_exp)

            # 2. M√©tricas da Decis√£o de Atualiza√ß√£o do Modelo
            update_decision_metrics = self._calculate_update_decision_metrics(df_exp)
            
            # 3. M√©tricas de Custo e Tempo
            # Usar .dropna() para evitar erros se houver NaNs
            median_agg_time = df_exp['aggregation_time'].dropna().median()
            std_agg_time = df_exp['aggregation_time'].dropna().std()
            total_time = df_exp['round_time'].dropna().sum()
            
            # Monta o dicion√°rio com todos os resultados finais para este experimento
            exp_summary = {
                'experiment_id': exp_id,
                'defense': final_row['defense'],
                'attack_rate': final_row['attack_rate'],
                'selection_fraction': final_row['selection_fraction'],
                'selection_strategy': final_row.get('selection_strategy'),
                'dataset_name': final_row.get('dataset_name'),
                'non_iid': final_row.get('non_iid'),
                'dirichlet_alpha': final_row.get('dirichlet_alpha'),
                'model_type': final_row.get('model_type'),
                'local_epochs': final_row.get('local_epochs'),
                'batch_size': final_row.get('batch_size'),
                'learning_rate': final_row.get('learning_rate'),
                'rounds_total': final_row.get('rounds_total'),
                'num_clients': final_row.get('num_clients'),
                'seed': final_row.get('seed'),

                # M√©tricas finais do modelo global
                'final_global_accuracy': final_row['final_accuracy'],
                'final_global_loss': final_row['final_loss'],
                'final_global_precision': final_row['final_precision'],
                'final_global_recall': final_row['final_recall'],
                'final_global_f1_score': final_row['final_f1_score'],
                
                # M√©tricas do filtro de clientes
                'filter_accuracy': filter_metrics['filter_accuracy'],
                'filter_precision': filter_metrics['filter_precision'],
                'filter_recall': filter_metrics['filter_recall'],
                'filter_f1_score': filter_metrics['filter_f1_score'],

                # M√©tricas da decis√£o de atualiza√ß√£o
                'update_decision_accuracy': update_decision_metrics['update_decision_accuracy'],
                'update_decision_FN_count': update_decision_metrics['update_decision_false_negatives'], # O mais cr√≠tico!

                # M√©tricas de custo
                'median_aggregation_time_s': median_agg_time,
                'std_aggregation_time_s': std_agg_time,
                'total_experiment_time_s': total_time,
                'total_communication_mb': df_exp['communication_bytes'].dropna().sum() / (1024*1024),
            }
            final_results.append(exp_summary)

        # Cria o DataFrame final e salva em um novo arquivo CSV
        df_final_summary = pd.DataFrame(final_results)
        output_path = self.results_dir / 'final_metrics_summary.csv'
        df_final_summary.to_csv(output_path, index=False, float_format='%.4f')
        
        print(f"‚úÖ Tabela de m√©tricas finais salva com sucesso em: {output_path}")



class ExperimentRunner:
    """
    Classe para gerar configura√ß√µes e preparar a simula√ß√£o de experimentos.
    """
    def __init__(self, resume_dir=None):
        self.defense_pipelines = {
            'Filtro Clustering + M√©dia Ponderada + Filtro L2 Global': {
                'client_filters': [{'name': 'CLUSTERING', 'params': {}}],
                'aggregation_strategy': {'name': 'FED_AVG'},
                'global_model_filter': {
                    'name': 'L2_GLOBAL_MODEL_FILTER',
                    'params': {'window_size': 3, 'std_dev_multiplier': 1.5, 'min_rounds_history': 3}
                }
            },
        }
        self.attack_rates = [0.0, 0.2, 0.4, 0.6, 0.8]
        self.selection_fractions = [0.4, 1.0]
        self.non_iid = [True, False]

        self.selection_strategy = 'random'
        self.model_name = 'CNN_SIGN'
        self.dataset_name = 'SIGN'
        self.rounds = 30
        self.num_clients = 10
        self.local_epochs = 2
        self.batch_size = 16
        # Defaults expl√≠citos para evitar omiss√µes silenciosas via suite
        self.learning_rate = 1e-4  # Pode ser lista no suite
        self.dirichlet_alpha = 0.1  # Pode ser lista no suite
        self.seed = 42

        self.save_client_models = False
        self.save_server_intermediate_models = False
        self.client_update_recorder = None

        if resume_dir:
            self.base_dir = Path(resume_dir)
            print(f"üîÑ Retomando simula√ß√£o no diret√≥rio: {self.base_dir}")
        else:
            self.base_dir = Path(f'simulations/simulation_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
            print(f"üÜï Iniciando nova simula√ß√£o em: {self.base_dir}")
        
        self.config_dir = self.base_dir / 'config'
        self.results_dir = self.base_dir / 'results'
        self.config_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.existing_exp_names = set()
        print(f"üöÄ Runner iniciado. Sa√≠da ser√° salva em: {self.base_dir}")

    def load_existing_results(self):
        """Varre o diret√≥rio de resultados para encontrar experimentos j√° conclu√≠dos."""
        print("\nüîç Verificando experimentos j√° conclu√≠dos...")
        if not self.results_dir.exists() or not any(self.results_dir.iterdir()):
            print("‚úÖ Nenhum resultado existente encontrado.")
            return

        # Usa o analisador apenas para carregar os dados, sem processar
        try:
            analyzer = ResultAnalyzer(self.base_dir)
            if analyzer.load_results():
                # Considera apenas experimentos marcados como sucesso
                self.existing_exp_names = {
                    name for name, v in analyzer.all_results.items()
                    if isinstance(v, dict) and v.get('success', False)
                }
            print(f"‚úÖ Encontrados {len(self.existing_exp_names)} resultados existentes.")
        except FileNotFoundError:
            print("‚úÖ Nenhum resultado existente encontrado (diret√≥rio 'results' n√£o existe).")

    def collect_configs_from_directory(self):
        """Coleta todos os arquivos YAML em self.config_dir como fonte de verdade em retomadas.

        Returns:
            Dict[str, Path]: mapping de experiment_name -> caminho do YAML
        """
        configs = {}
        if not self.config_dir.exists():
            return configs
        for cfg_path in sorted(self.config_dir.glob('*.yaml')):
            try:
                exp_name = cfg_path.stem
                configs[exp_name] = cfg_path
            except Exception:
                # Pula qualquer arquivo estranho
                continue
        return configs

    def generate_all_configs(self):
        """Gera um dicion√°rio com todas as configura√ß√µes de experimentos planejados."""
        planned_configs = {}
        for defense_name, defense_pipeline in self.defense_pipelines.items():
            for selection_strategy in ensure_list(self.selection_strategy):
                for model_name in ensure_list(self.model_name):
                    for dataset_name in ensure_list(self.dataset_name):
                        for rounds in ensure_list(self.rounds):
                            for num_clients in ensure_list(self.num_clients):
                                for local_epochs in ensure_list(self.local_epochs):
                                    for batch_size in ensure_list(self.batch_size):
                                        lr_list = ensure_list(self.learning_rate) if self.learning_rate is not None else [None]
                                        for learning_rate in lr_list:
                                            for seed in ensure_list(self.seed):
                                                for attack_rate in ensure_list(self.attack_rates):
                                                    for sel_frac in ensure_list(self.selection_fractions):
                                                        for non_iid in ensure_list(self.non_iid):
                                                            # Se for IID, dirichlet_alpha n√£o se aplica
                                                            if bool(non_iid):
                                                                alpha_list = ensure_list(self.dirichlet_alpha) if self.dirichlet_alpha is not None else [None]
                                                            else:
                                                                alpha_list = [None]
                                                            for dir_alpha in alpha_list:
                                                                # Nome do experimento contendo todas varia√ß√µes relevantes
                                                                attack_part = f"lf_{int(attack_rate*100)}" if attack_rate and attack_rate > 0 else "no_attack"
                                                                defense_part = slugify(defense_name)
                                                                sf_part = f"sf_{int(sel_frac*100)}"
                                                                strat_part = f"sel_{slugify(str(selection_strategy))}"
                                                                ds_part = f"ds_{slugify(str(dataset_name))}"
                                                                model_part = f"m_{slugify(str(model_name))}"
                                                                rounds_part = f"r_{rounds}"
                                                                clients_part = f"n_{num_clients}"
                                                                epochs_part = f"ep_{local_epochs}"
                                                                bs_part = f"bs_{batch_size}"
                                                                lr_part = f"lr_{learning_rate}" if learning_rate is not None else "lr_default"
                                                                seed_part = f"s_{seed}"
                                                                niid_part = 'non_iid' if non_iid else 'iid'
                                                                alpha_part = f"alpha_{dir_alpha}" if (bool(non_iid) and dir_alpha is not None) else None
                                                                name_parts = [attack_part, defense_part, sf_part, strat_part, ds_part, model_part, rounds_part, clients_part, epochs_part, bs_part, lr_part, seed_part, niid_part]
                                                                if alpha_part:
                                                                    name_parts.append(alpha_part)
                                                                exp_name = "_".join(name_parts)

                                                                config = {
                                                                    'experiment': {
                                                                        'name': exp_name,
                                                                        'defense_name': defense_name,
                                                                        'description': f"{defense_name} vs {int((attack_rate or 0)*100)}% label flipping (sel: {int(sel_frac*100)}%)",
                                                                        'seed': seed,
                                                                        'rounds': rounds,
                                                                        'output_dir': str(self.results_dir),
                                                                        'log_level': 'info',
                                                                        'save_client_models': self.save_client_models,
                                                                        'save_server_intermediate_models': self.save_server_intermediate_models,
                                                                        'reuse_client_model': getattr(self, 'reuse_client_model', False),
                                                                        'keras_verbose': int(getattr(self, 'keras_verbose', 0))
                                                                            },
                                                                    'dataset': {
                                                                        'name': dataset_name,
                                                                        'non_iid': bool(non_iid),
                                                                        'dirichlet_alpha': float(dir_alpha) if (bool(non_iid) and dir_alpha is not None) else None
                                                                    },
                                                                    'model': {
                                                                        'type': model_name,
                                                                        'local_epochs': int(local_epochs),
                                                                        'batch_size': int(batch_size)
                                                                    },
                                                                    'server': {
                                                                        'type': 'standard',
                                                                        'address': '0.0.0.0:8000',
                                                                        'defense_pipeline': defense_pipeline,
                                                                        'selection_strategy': selection_strategy,
                                                                        'selection_fraction': float(sel_frac),
                                                                        'evaluation_interval': 1,
                                                                        'max_concurrent_clients': int(getattr(self, 'max_concurrent_clients', 3))
                                                                    },
                                                                    'clients': {
                                                                        'num_clients': int(num_clients),
                                                                        'honest_client_type': 'standard',
                                                                        'malicious_client_type': 'label_flipping' if (attack_rate and attack_rate > 0) else None,
                                                                        'malicious_percentage': float(attack_rate or 0.0)
                                                                    }
                                                                }
                                                                if learning_rate is not None:
                                                                    config['model']['learning_rate'] = float(learning_rate)
                                                                if self.client_update_recorder:
                                                                    config['experiment']['client_update_recorder'] = self.client_update_recorder
                                                                planned_configs[exp_name] = config
        return planned_configs

    def estimate_time(self, configs):
        """Estima tempo total baseado em experi√™ncia anterior."""
        num_pending_experiments = len(configs)
        if num_pending_experiments == 0:
            return 0
        
        # Estimativas baseadas no tipo de modelo por round completo
        # O coment√°rio "considerando 100 clientes" √© a chave
        model_time_estimates_per_round = {
            'CNN_MNIST': 6,  # segundos para 100 clientes
            'DNN_MNIST': 2,  # segundos para 100 clientes
        }
        N_BASE_CLIENTS = 100.0  # Baseline de clientes para as estimativas acima
        DEFAULT_TIME = 6.0     # Tempo padr√£o se o modelo n√£o for encontrado

        # 1. Calcular o tempo m√©dio por cliente, com base nos modelos nos configs
        # (Se configs for apenas uma lista de nomes, usar os defaults do runner)
        model_types_in_suite = ensure_list(self.model_name)
        model_times_per_client = []
        for model_type in model_types_in_suite:
             # Obter o tempo base (para 100 clientes) ou usar o padr√£o
            time_base_100_clients = model_time_estimates_per_round.get(model_type, DEFAULT_TIME)
            # Calcular o tempo por cliente individual
            time_per_client = time_base_100_clients / N_BASE_CLIENTS
            model_times_per_client.append(time_per_client)

        if not model_times_per_client:
            return 0  # N√£o deveria acontecer se num_pending_experiments > 0, mas √© seguro

        avg_time_per_client_per_round = np.mean(model_times_per_client)

        # 2. Obter par√¢metros m√©dios da su√≠te (corrigindo bugs de 'None')
        # Preservando sua l√≥gica 'ensure_list'
        avg_selection_fraction = float(np.mean(ensure_list(self.selection_fractions))) if self.selection_fractions is not None else 0.0
        avg_num_clients = float(np.mean(ensure_list(self.num_clients))) if self.num_clients is not None else 0.0
        avg_rounds = float(np.mean(ensure_list(self.rounds))) if self.rounds is not None else 0.0

        # 3. Calcular o tempo com base na nova l√≥gica (corrigida)
        avg_clients_per_round = avg_num_clients * avg_selection_fraction
        
        # Esta √© a nova l√≥gica principal:
        avg_time_per_round = avg_clients_per_round * avg_time_per_client_per_round
        
        avg_time_per_experiment = (avg_rounds * avg_time_per_round) + 60  # Adiciona 60 segundos de overhead fixo por experimento
        
        total_seconds = num_pending_experiments * avg_time_per_experiment
        hours = total_seconds / 3600
        
        # 4. Imprimir o relat√≥rio
        print(f"‚è±Ô∏è  Estimativa de Tempo:")
        print(f"  - üìä {num_pending_experiments} experimentos pendentes")
        print(f"  - üîÑ {int(avg_rounds)} rounds por experimento (m√©dia)")
        print(f"  - üë• {avg_clients_per_round:.1f} clientes por round (m√©dia)")
        print(f"  - ‚è±Ô∏è  ~{avg_time_per_round:.2f} seg por round (m√©dia)")
        print(f"  - ‚è±Ô∏è  ~{avg_time_per_experiment/60:.1f} min por experimento (em m√©dia)")
        print(f"  - üïê Tempo total estimado: {hours:.1f} horas ({hours/24:.1f} dias)")
        
        if hours > 8:
            print(f"  - ‚ö†Ô∏è  A execu√ß√£o pode ser longa. Considere executar em partes ou usar uma m√°quina mais r√°pida.")
        
        return total_seconds
    
    # run_workflow removido: fluxo de execu√ß√£o agora √© externo
    # save_results_summary removido: movido para ResultAnalyzer


# --- IN√çCIO DAS FUN√á√ïES AUXILIARES DE PREPARA√á√ÉO ---

def generate_job_scripts(runner: ExperimentRunner, pending_list: list, jobs: int, sim_jobs: int, tf_threads: int | None):
    """
    Divide os configs pendentes em scripts de shell para execu√ß√£o paralela.
    """
    if not pending_list:
        print("‚úÖ Nenhum experimento pendente encontrado. Nada a fazer.")
        return

    # Caminho para o script executor
    simulate_path = Path(__file__).resolve().parent / 'simulate_fl.py'
    if not simulate_path.exists():
        print(f"‚ùå ERRO: 'simulate_fl.py' n√£o encontrado em {simulate_path.parent}")
        return

    # Divide a lista de arquivos de config pendentes em 'jobs' lotes
    job_batches = np.array_split(pending_list, jobs)
    
    print(f"‚öñÔ∏è  Dividindo {len(pending_list)} configs em {len(job_batches)} jobs...")

    for i, batch in enumerate(job_batches):
        if len(batch) == 0:
            continue # Evita criar jobs vazios se jobs > pending_list
            
        job_num = i + 1
        script_name = f"run_job_{job_num}.sh"
        script_path = runner.base_dir / script_name

        with open(script_path, 'w') as f:
            f.write("#!/bin/bash\n")
            f.write(f"echo '--- ‚è≥ Iniciando Job {job_num}/{jobs} -- (Processando {len(batch)} configs) ---'\n")
            
            # Constr√≥i o comando
            cmd_parts = [
                sys.executable, 
                str(simulate_path), 
                '--config'
            ] + [str(p.relative_to(runner.base_dir)) for p in batch] # Usa caminhos relativos ao base_dir
            
            if tf_threads is not None:
                cmd_parts.extend(['--tf-threads', str(tf_threads)])
            else:
                # Default conservador se n√£o for passado
                cmd_parts.extend(['--tf-threads', '1'])
                
            # PASSE OS JOBS INTERNOS PARA O SIMULATE_FL.PY
            cmd_parts.extend(['--jobs', str(sim_jobs)])
            
            # Adiciona --jobs 1 ao simulate_fl.py para garantir que ele rode sequencialmente *dentro* do job
            # (O paralelismo de jobs √© gerenciado pelos terminais)
            # NOTA: Se simulate_fl.py --jobs > 1 for desejado *dentro* de um worker, ajuste aqui.
            # Por padr√£o, vamos manter 1 para logs mais limpos por worker.
            # cmd_parts.extend(['--jobs', '1']) 

            f.write(" ".join(cmd_parts) + "\n")
            f.write(f"echo '\n--- ‚úÖ Job {job_num}/{jobs} Conclu√≠do ---'\n")
            f.write("read -p 'Pressione Enter para fechar este terminal...'\n")
        
        # Torna o script execut√°vel
        os.chmod(script_path, 0o755)
        print(f"  -> üìú Script '{script_name}' criado com {len(batch)} configs.")

def print_job_instructions(base_dir: Path, jobs: int):
    """
    Imprime as instru√ß√µes finais para o usu√°rio executar os scripts de job.
    """
    print("\n" + "="*60)
    print("‚úÖ PREPARA√á√ÉO CONCLU√çDA ‚úÖ")
    print("="*60)
    print(f"Simula√ß√£o pronta em: {base_dir.resolve()}")
    print(f"Foram criados {jobs} scripts de job para execu√ß√£o.")
    print("\n‚û°Ô∏è  Pr√≥ximos passos:\n")
    print(f"1. Abra {jobs} novos terminais.")
    print(f"2. Em cada terminal, navegue at√© o diret√≥rio e execute um dos scripts:")
    
    # Mostra comandos f√°ceis de copiar e colar
    base_dir_str = str(base_dir.resolve())
    for i in range(1, jobs + 1):
        print(f"\n   --- Terminal {i} ---")
        print(f"   cd \"{base_dir_str}\"")
        print(f"   ./run_job_{i}.sh")

    print(f"\n3. Quando TODOS os jobs terminarem (e voc√™ pressionar Enter neles):")
    print(f"4. Volte para este terminal e rode a an√°lise final:")
    
    # Constr√≥i o comando de an√°lise
    analyze_cmd = f"python3 \"{Path(__file__).resolve()}\" --analyze_dir \"{base_dir_str}\""
    print(f"\n   {analyze_cmd}\n")
    print("="*60 + "\n")


# --- FIM DAS FUN√á√ïES AUXILIARES ---


def main():
    parser = argparse.ArgumentParser(
        description="Script para PREPARAR e ANALISAR experimentos de defesas em FL.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        '--run',
        action='store_true',
        help='Modo Prepara√ß√£o: Prepara uma *nova* simula√ß√£o (gera configs e scripts de job).'
    )
    group.add_argument(
        '--analyze_dir',
        type=str,
        metavar='PATH',
        help='Modo An√°lise: Analisa resultados de uma simula√ß√£o *conclu√≠da*.\n'
             'Exemplo: --analyze_dir simulation_20250912_011458'
    )
    group.add_argument(
        '--resume_dir',
        type=str,
        metavar='PATH',
        help='Modo Prepara√ß√£o (Retomar): Prepara a *retomada* de uma simula√ß√£o.\n'
             'Exemplo: --resume_dir simulation_20250912_011458'
    )
    parser.add_argument(
        '--suite',
        type=str,
        metavar='PATH',
        help='(Prepara√ß√£o) Arquivo YAML mestre descrevendo todos os experimentos.'
    )
    parser.add_argument(
        '--jobs',
        type=int,
        default=1,
        help='(Prepara√ß√£o) N√∫mero de scripts de job paralelos para gerar.'
    )
    parser.add_argument(
        '--threads',
        action='store_true',
        help='(Prepara√ß√£o) Passa a flag --threads para simulate_fl.py (paralelismo *dentro* da simula√ß√£o).'
    )
    parser.add_argument(
        '--tf-threads',
        type=int,
        default=None,
        help='(Prepara√ß√£o) Passa a flag --tf-threads para simulate_fl.py (override de threads do TF).'
    )
    args = parser.parse_args()

    # --- MODO DE AN√ÅLISE ---
    if args.analyze_dir:
        print("--- üìä MODO DE AN√ÅLISE ATIVADO ---")
        try:
            analyzer = ResultAnalyzer(args.analyze_dir)
            analyzer.run_full_analysis()
            print(f"\n‚úÖ An√°lise conclu√≠da com sucesso para: {args.analyze_dir}")
        except FileNotFoundError as e:
            print(f"‚ùå Erro: {e}")
        except Exception as e:
            print(f"‚ùå Erro inesperado durante a an√°lise: {e}")
            
    # --- MODO DE PREPARA√á√ÉO (NOVO OU RETOMADA) ---
    else:
        # Verifica√ß√£o antecipada do simulador
        try:
            from fl_simulator import FLSimulator
            if FLSimulator is None:
                raise ImportError("FLSimulator √© None")
        except Exception:
            print("‚ùå ERRO: N√£o foi poss√≠vel importar FLSimulator.")
            print("   O modo de prepara√ß√£o (--run) e retomada (--resume_dir) n√£o funcionar√£o.")
            print("   Verifique sua instala√ß√£o e o PYTHONPATH.")
            sys.exit(1)

        print("--- üõ†Ô∏è  MODO DE PREPARA√á√ÉO ATIVADO ---")
        
        # '--run' precisa de uma suite
        if args.run and not args.suite:
            print("‚ùå Erro: O modo --run requer um arquivo --suite.")
            sys.exit(1)

        # Se resume_dir for fornecido, ele continua. Se for --run, resume_dir √© None e inicia uma nova.
        runner = ExperimentRunner(resume_dir=args.resume_dir)

        # Se um arquivo de suite for fornecido, sobrep√µe as op√ß√µes do runner
        if args.suite:
            with open(args.suite, 'r') as f:
                suite_cfg = yaml.safe_load(f)

            # Atribui com defaults seguros
            runner.defense_pipelines = suite_cfg.get('defense_pipelines', runner.defense_pipelines)
            runner.attack_rates = suite_cfg.get('attack_rates', runner.attack_rates)
            runner.selection_fractions = suite_cfg.get('selection_fractions', runner.selection_fractions)
            runner.non_iid = suite_cfg.get('non_iid', runner.non_iid)
            runner.dirichlet_alpha = suite_cfg.get('dirichlet_alpha', runner.dirichlet_alpha)

            runner.selection_strategy = suite_cfg.get('selection_strategy', runner.selection_strategy)
            runner.model_name = suite_cfg.get('model_name', runner.model_name)
            runner.dataset_name = suite_cfg.get('dataset_name', runner.dataset_name)
            runner.rounds = suite_cfg.get('rounds', runner.rounds)
            runner.num_clients = suite_cfg.get('num_clients', runner.num_clients)
            runner.local_epochs = suite_cfg.get('local_epochs', runner.local_epochs)
            runner.batch_size = suite_cfg.get('batch_size', runner.batch_size)
            runner.learning_rate = suite_cfg.get('learning_rate', runner.learning_rate)
            runner.seed = suite_cfg.get('seed', runner.seed)

            runner.save_client_models = suite_cfg.get('save_client_models', runner.save_client_models)
            runner.save_server_intermediate_models = suite_cfg.get('save_server_intermediate_models', runner.save_server_intermediate_models)
            runner.client_update_recorder = suite_cfg.get('client_update_recorder', runner.client_update_recorder)

            # Extras passados via experiment
            runner.reuse_client_model = suite_cfg.get('reuse_client_model', False)
            runner.keras_verbose = int(suite_cfg.get('keras_verbose', 0))
            runner.max_concurrent_clients = suite_cfg.get('max_concurrent_clients', 8)

        # 1. Carregar o que j√° foi feito
        runner.load_existing_results()

        # 2. Carregar o que *deveria* ser feito (do disco ou da suite)
        
        # Em retomadas, por padr√£o usamos os YAMLs j√° existentes na pasta config/
        if args.resume_dir:
            planned_configs_files = runner.collect_configs_from_directory()
            if not planned_configs_files:
                print("‚ö†Ô∏è Nenhum YAML encontrado em config/. Gerando configura√ß√µes a partir dos par√¢metros do runner.")
                # ... (l√≥gica de gera√ß√£o e persist√™ncia de configs) ...
                planned_configs = runner.generate_all_configs()
                for name, config_data in planned_configs.items():
                    config_path = runner.config_dir / f"{name}.yaml"
                    config_data['experiment']['reuse_client_model'] = getattr(runner, 'reuse_client_model', False)
                    config_data['experiment']['keras_verbose'] = int(getattr(runner, 'keras_verbose', 0))
                    config_data['server']['max_concurrent_clients'] = getattr(runner, 'max_concurrent_clients', 3)
                    with open(config_path, 'w') as f:
                        yaml.dump(config_data, f, default_flow_style=False, sort_keys=False)
                planned_configs_files = runner.collect_configs_from_directory()

            # Se um suite foi passado junto com resume_dir, mescla
            if args.suite:
                print("üß© Mesclando novas configura√ß√µes do suite na simula√ß√£o existente...")
                suite_planned = runner.generate_all_configs()
                new_count = 0
                for name, config_data in suite_planned.items():
                    if name not in planned_configs_files:
                        config_path = runner.config_dir / f"{name}.yaml"
                        # Injetar flags extras
                        config_data['experiment']['reuse_client_model'] = getattr(runner, 'reuse_client_model', False)
                        config_data['experiment']['keras_verbose'] = int(getattr(runner, 'keras_verbose', 0))
                        config_data['server']['max_concurrent_clients'] = getattr(runner, 'max_concurrent_clients', 3)
                        with open(config_path, 'w') as f:
                            yaml.dump(config_data, f, default_flow_style=False, sort_keys=False)
                        planned_configs_files[name] = config_path
                        new_count += 1
                print(f"‚úÖ {new_count} novas configs adicionadas a {runner.config_dir} a partir do suite.")

            # Carrega resultados completos para identificar incompletos
            analyzer_for_pending = ResultAnalyzer(runner.base_dir)
            analyzer_for_pending.load_results()
            incomplete_exps = {
                name for name, v in analyzer_for_pending.all_results.items()
                if isinstance(v, dict) and not v.get('success', False)
            }

            # Calcula conclu√≠dos entre os planejados (com base nos resultados de sucesso)
            completed_among_planned = sum(1 for name in planned_configs_files.keys() if name in runner.existing_exp_names)

            # Pendentes: n√£o conclu√≠dos com sucesso OU marcados como incompletos
            pending_files = {
                name: path for name, path in planned_configs_files.items()
                if (name not in runner.existing_exp_names) or (name in incomplete_exps)
            }
            pending_list = list(pending_files.values())

            print(f"\nüìä Status da Simula√ß√£o (Retomada):")
            print(f"  - {len(planned_configs_files)} configs encontrados em {runner.config_dir}.")
            print(f"  - {completed_among_planned} j√° conclu√≠dos com sucesso entre os planejados.")
            print(f"  - {len(pending_files)} pendentes para execu√ß√£o (inclui incompletos para rein√≠cio).")

        # Fluxo original para novas execu√ß√µes
        else:
            planned_configs = runner.generate_all_configs()
            pending_configs = {
                name: cfg for name, cfg in planned_configs.items()
                if name not in runner.existing_exp_names
            }

            print(f"\nüìä Status da Simula√ß√£o (Nova):")
            print(f"  - {len(planned_configs)} experimentos planejados.")
            print(f"  - {len(runner.existing_exp_names)} experimentos j√° conclu√≠dos (de execu√ß√µes anteriores no mesmo dir?).")
            print(f"  - {len(pending_configs)} experimentos pendentes para execu√ß√£o.")
            
            # Escreve todos os YAMLs pendentes
            pending_list = []
            for name, config_data in pending_configs.items():
                config_path = runner.config_dir / f"{name}.yaml"
                # Injetar flags extras de otimiza√ß√£o
                config_data['experiment']['reuse_client_model'] = getattr(runner, 'reuse_client_model', False)
                config_data['experiment']['keras_verbose'] = int(getattr(runner, 'keras_verbose', 0))
                config_data['server']['max_concurrent_clients'] = getattr(runner, 'max_concurrent_clients', 3)
                with open(config_path, 'w') as f:
                    yaml.dump(config_data, f, default_flow_style=False, sort_keys=False)
                pending_list.append(config_path)

        # 3. Estimar e Confirmar (Comum a --run e --resume_dir)
        if pending_list:
            runner.estimate_time({p.stem: {} for p in pending_list}) # Estima usando a lista pendente
            
            print("-" * 40)
            print(f"Ser√£o gerados {args.jobs} scripts de job para processar {len(pending_list)} configs.")
            response = input(f"‚ùì Deseja continuar e preparar os scripts? (y/N): ").lower().strip()
            print("-" * 40)

            if response == 'y':
                print(f"\nüõ†Ô∏è  Preparando scripts de job...")
                
                num_job_scripts = args.jobs         # Agora N scripts (ex: 2)
                internal_sim_jobs = 1             # Cada script usar√° --jobs 1

                print(f"\nüõ†Ô∏è  Preparando {num_job_scripts} script(s) de job...")
                print(f"   (Cada script usar√° {internal_sim_jobs} processos paralelos internos via simulate_fl.py)")

                generate_job_scripts(
                    runner=runner,
                    pending_list=pending_list,
                    jobs=num_job_scripts,         # Passa 2 para gerar run_job_1.sh, run_job_2.sh
                    sim_jobs=internal_sim_jobs,  # Passa 1 para o comando dentro do script
                    tf_threads=args.tf_threads
                )

                print_job_instructions(runner.base_dir, num_job_scripts) # Passa 2 para instru√ß√µes
                
            else:
                print("‚ùå Prepara√ß√£o cancelada pelo usu√°rio.")
        else:
            print("\n‚úÖ Nenhum experimento pendente. Nada a preparar.")
            print(f"Voc√™ pode rodar a an√°lise agora, se desejar:")
            print(f"python3 \"{Path(__file__).resolve()}\" --analyze_dir \"{runner.base_dir.resolve()}\"")
            
        # O script de prepara√ß√£o termina aqui. A execu√ß√£o e an√°lise s√£o passos separados.


if __name__ == "__main__":
    main()